{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J cola-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 1\n",
    "#SBATCH -c 5\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 0:59:59\n",
    "#SBATCH --output=eval_cola_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/cola_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 1 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --no_pad_to_max_length \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name cola \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --load_best_model_at_end \\\\\n",
    "        --metric_for_best_model \"matthews_correlation\" \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 17800\n",
    "i = 0\n",
    "for bs in [8, 16, 32, 64]:\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        epoch = 8\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_cola_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J mnli-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 4\n",
    "#SBATCH -c 20\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 3:59:59\n",
    "#SBATCH --output=eval_mnli_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/mnli_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 4 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --no_pad_to_max_length \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name mnli \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --load_best_model_at_end \\\\\n",
    "        --metric_for_best_model \"accuracy\" \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 17900\n",
    "i = 0\n",
    "for bs in [8, 16]:\n",
    "    epoch = 5\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_mnli_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J mrpc-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 4\n",
    "#SBATCH -c 20\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 0:59:59\n",
    "#SBATCH --output=eval_mrpc_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/mrpc_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 4 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --no_pad_to_max_length \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name mrpc \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --load_best_model_at_end \\\\\n",
    "        --metric_for_best_model \"f1\" \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 18000\n",
    "i = 0\n",
    "for bs in [2, 4, 8, 16]:\n",
    "    epoch = 5\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_mrpc_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J qnli-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 4\n",
    "#SBATCH -c 20\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 3:59:59\n",
    "#SBATCH --output=eval_qnli_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/qnli_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 4 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --max_seq_length 512 \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name qnli \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --load_best_model_at_end \\\\\n",
    "        --metric_for_best_model \"accuracy\" \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 18100\n",
    "i = 0\n",
    "for bs in [2, 4, 8, 16]:\n",
    "    epoch = 5\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_qnli_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J wnli-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 1\n",
    "#SBATCH -c 5\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 0:59:59\n",
    "#SBATCH --output=eval_wnli_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/wnli_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 1 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --no_pad_to_max_length \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name wnli \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 18200\n",
    "i = 0\n",
    "for bs in [8, 16, 32, 64]:\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        epoch = 8\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_wnli_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J stsb-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 4\n",
    "#SBATCH -c 20\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 0:59:59\n",
    "#SBATCH --output=eval_stsb_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/stsb_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 4 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --no_pad_to_max_length \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name stsb \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --load_best_model_at_end \\\\\n",
    "        --metric_for_best_model \"combined_score\" \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 18300\n",
    "i = 0\n",
    "for bs in [2, 4, 8, 16]:\n",
    "    epoch = 5\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_stsb_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J rte-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 4\n",
    "#SBATCH -c 20\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 0:59:59\n",
    "#SBATCH --output=eval_rte_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/rte_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 4 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --no_pad_to_max_length \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name rte \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --load_best_model_at_end \\\\\n",
    "        --metric_for_best_model \"accuracy\" \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 18400\n",
    "i = 0\n",
    "for bs in [2, 4, 8, 16]:\n",
    "    epoch = 5\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_rte_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J qqp-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 4\n",
    "#SBATCH -c 20\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 3:59:59\n",
    "#SBATCH --output=eval_qqp_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/qqp_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 4 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --no_pad_to_max_length \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name qqp \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --load_best_model_at_end \\\\\n",
    "        --metric_for_best_model \"combined_score\" \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 18500\n",
    "i = 0\n",
    "for bs in [8, 16]:\n",
    "    epoch = 5\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_qqp_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -A m3624\n",
    "#SBATCH -C gpu\n",
    "#SBATCH -J sst2-{cased}-{bs}-{epoch}\n",
    "#SBATCH -G 4\n",
    "#SBATCH -c 20\n",
    "#SBATCH --mail-user=haoyan.huo@lbl.gov\n",
    "#SBATCH -t 3:59:59\n",
    "#SBATCH --output=eval_sst2_{cased}_bs{bs}_epoch{epoch}.out\n",
    "## DISABLED: SBATCH --mail-type=END,FAIL\n",
    "\n",
    "module load cuda\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate bert\n",
    "\n",
    "export HF_HOME=$SCRATCH/cache/huggingface\n",
    "export TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "export OUTPUTDIR=$SCRATCH/glue_eval/sst2_{cased}_bs{bs}_epoch{epoch}\n",
    "mkdir -p $OUTPUTDIR\n",
    "echo \"Starting to train cased, batch size {bs}, epochs {epoch} at $(date)\"\n",
    "time python -m torch.distributed.launch --nnodes 1 --nproc_per_node 4 --master_addr 127.0.0.1 --master_port {port} \\\n",
    "    evaluate_glue.py --overwrite_output_dir \\\\\n",
    "        --no_pad_to_max_length \\\\\n",
    "        --dataloader_num_workers 4 \\\\\n",
    "        --model_name_or_path ../models/matbert-base-{cased}/ {lower_case}\\\\\n",
    "        --per_device_train_batch_size {bs} \\\\\n",
    "        --num_train_epochs {epoch} \\\\\n",
    "        --output_dir $OUTPUTDIR \\\\\n",
    "        --logging_dir $OUTPUTDIR/run \\\\\n",
    "        --task_name sst2 \\\\\n",
    "        --do_train --do_eval --do_predict \\\\\n",
    "        --load_best_model_at_end \\\\\n",
    "        --metric_for_best_model \"accuracy\" \\\\\n",
    "        --seed 42 \\\\\n",
    "        --evaluation_strategy 'epoch' > $OUTPUTDIR/file.log 2>&1 \n",
    "echo \"Finished at $(date)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_port = 18600\n",
    "i = 0\n",
    "for bs in [2, 4, 8, 16]:\n",
    "    epoch = 5\n",
    "    for cased in ['cased', 'uncased']:\n",
    "        lower_case = '--no_do_lower_case ' if cased == 'cased' else ''\n",
    "        script = template.format(bs=bs, epoch=epoch, cased=cased, lower_case=lower_case, port=base_port+i)\n",
    "        i += 1\n",
    "\n",
    "        fn = 'script_sst2_{cased}_bs{bs}_epoch{epoch}.sh'.format(bs=bs, epoch=epoch, cased=cased)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
